{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ee01537",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Estimation of treatment efficacy of real-world clinical interventions involves working with continuous outcomes such as time-to-death, re-hospitalization, or a composite event that may be subject to censoring. Causal reasoning in such scenarios requires decoupling the effects of confounding physiological characteristics that affect baseline survival rates from the effects of the interventions being assessed. In this paper, we present a latent variable approach to model heterogeneous treatment effects by proposing that an individual can belong to one of latent clusters with distinct response characteristics. We show that this latent structure can mediate the base survival rates and helps determine the effects of an intervention. We demonstrate the ability of our approach to discover actionable phenotypes of individuals based on their treatment response on multiple large randomized clinical trials originally conducted to assess appropriate treatment strategies to reduce cardiovascular risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfde8db",
   "metadata": {},
   "source": [
    "## 2. Synthetic Data Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a99d9a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../auton_survival/')\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the synthetic dataset\n",
    "outcomes, features, interventions = load_dataset(dataset='SYNTHETIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04ec4a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.148745</td>\n",
       "      <td>1.892484</td>\n",
       "      <td>0.195254</td>\n",
       "      <td>0.860757</td>\n",
       "      <td>0.696523</td>\n",
       "      <td>0.483697</td>\n",
       "      <td>0.339551</td>\n",
       "      <td>0.374794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.139439</td>\n",
       "      <td>-0.943330</td>\n",
       "      <td>0.411054</td>\n",
       "      <td>0.179533</td>\n",
       "      <td>0.428686</td>\n",
       "      <td>0.683057</td>\n",
       "      <td>0.600948</td>\n",
       "      <td>0.070483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.961237</td>\n",
       "      <td>0.782706</td>\n",
       "      <td>-0.305381</td>\n",
       "      <td>0.583576</td>\n",
       "      <td>0.157478</td>\n",
       "      <td>0.070556</td>\n",
       "      <td>0.034590</td>\n",
       "      <td>0.776005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.466508</td>\n",
       "      <td>0.694348</td>\n",
       "      <td>-0.249651</td>\n",
       "      <td>1.567092</td>\n",
       "      <td>0.850959</td>\n",
       "      <td>0.416178</td>\n",
       "      <td>0.968841</td>\n",
       "      <td>0.863598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.249002</td>\n",
       "      <td>-0.552091</td>\n",
       "      <td>1.854651</td>\n",
       "      <td>-0.466234</td>\n",
       "      <td>0.860385</td>\n",
       "      <td>0.367184</td>\n",
       "      <td>0.954347</td>\n",
       "      <td>0.748930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1        X2        X3        X4        X5        X6        X7  \\\n",
       "0  0.148745  1.892484  0.195254  0.860757  0.696523  0.483697  0.339551   \n",
       "1  1.139439 -0.943330  0.411054  0.179533  0.428686  0.683057  0.600948   \n",
       "2 -0.961237  0.782706 -0.305381  0.583576  0.157478  0.070556  0.034590   \n",
       "3  0.466508  0.694348 -0.249651  1.567092  0.850959  0.416178  0.968841   \n",
       "4 -0.249002 -0.552091  1.854651 -0.466234  0.860385  0.367184  0.954347   \n",
       "\n",
       "         X8  \n",
       "0  0.374794  \n",
       "1  0.070483  \n",
       "2  0.776005  \n",
       "3  0.863598  \n",
       "4  0.748930  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at take the dataset\n",
    "features.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "173ec140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fc4e40",
   "metadata": {},
   "source": [
    "### Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f7f70b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data points: 3899\n",
      "Number of test data points: 1101\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "random_seed = 0\n",
    "test_size = 0.25\n",
    "\n",
    "# Split the synthetic data into training and testing data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "n = features.shape[0] \n",
    "\n",
    "test_idx = np.zeros(n).astype('bool')\n",
    "test_idx[np.random.randint(n, size=int(n*test_size))] = True \n",
    "\n",
    "features_tr = features.iloc[~test_idx] \n",
    "outcomes_tr = outcomes.iloc[~test_idx]\n",
    "interventions_tr = interventions[~test_idx]\n",
    "print(f'Number of training data points: {len(features_tr)}')\n",
    "\n",
    "features_te = features.iloc[test_idx] \n",
    "outcomes_te = outcomes.iloc[test_idx]\n",
    "interventions_te = interventions[test_idx]\n",
    "print(f'Number of test data points: {len(features_te)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd7fe50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters to train model\n",
    "\n",
    "k = 2 # Number of underlying base survival phenotypes\n",
    "g = 2 # Number of underlying treatment effect phenotypes\n",
    "layers = [50] # Number of neurons in each hidden layer.\n",
    "\n",
    "model_random_seed = 0\n",
    "epochs = 50\n",
    "lr = 1e-3\n",
    "bs = 128\n",
    "vsize = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "723352b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of covariates: (3899, 8) | times: (3899,) | events: (3899,) | interventions: (3899,)\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'act' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_38253/1988210413.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeepCoxMixturesHeterogenousEffects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m model = model.fit(x, t, e, a, vsize=0.15, val_data=None, iters=50, learning_rate=1e-3, \n\u001b[0m\u001b[1;32m     20\u001b[0m                   batch_size=128, optimizer=\"Adam\", random_state=0)\n",
      "\u001b[0;32m~/auton-survival/examples/../auton_survival/models/cmhe/__init__.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, t, e, a, vsize, val_data, iters, learning_rate, batch_size, optimizer, random_state)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0minputdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gen_torch_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     model, _ = train_cmhe(model,\n",
      "\u001b[0;32m~/auton-survival/examples/../auton_survival/models/cmhe/__init__.py\u001b[0m in \u001b[0;36m_gen_torch_model\u001b[0;34m(self, inputdim, optimizer)\u001b[0m\n\u001b[1;32m    171\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_gen_torch_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;34m\"\"\"Helper function to return a torch model.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     return DeepCMHETorch(self.k, self.g, inputdim,\n\u001b[0m\u001b[1;32m    174\u001b[0m                          \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                          optimizer=optimizer)\n",
      "\u001b[0;32m~/auton-survival/examples/../auton_survival/models/cmhe/cmhe_torch.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, k, g, inputdim, layers, optimizer)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_dcmhe_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlastdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_representation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Tanh'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/auton-survival/examples/../dsm/dsm_torch.py\u001b[0m in \u001b[0;36mcreate_representation\u001b[0;34m(inputdim, layers, activation)\u001b[0m\n\u001b[1;32m     86\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mprevdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'act' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set torch and numpy random seeds\n",
    "torch.manual_seed(model_random_seed)\n",
    "np.random.seed(model_random_seed)\n",
    "\n",
    "# Convert training data into torch tensors\n",
    "x = features_tr.values.astype('float32')\n",
    "t = outcomes_tr['time'].values.astype('float32')\n",
    "e = outcomes_tr['event'].values.astype('float32')\n",
    "a = interventions_tr.values.astype('float32')\n",
    "print(f'Shape of covariates: {x.shape} | times: {t.shape} | events: {e.shape} | interventions: {a.shape}')\n",
    "\n",
    "from models.cmhe import DeepCoxMixturesHeterogenousEffects\n",
    "\n",
    "# Instantiate the CMHE model\n",
    "model = DeepCoxMixturesHeterogenousEffects(k=k, g=g, layers=layers)\n",
    "\n",
    "model = model.fit(x, t, e, a, vsize=0.15, val_data=None, iters=50, learning_rate=1e-3, \n",
    "                  batch_size=128, optimizer=\"Adam\", random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1c0102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90ed579",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Training !!!!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if len(layers): model = DeepCoxSubgroupMixture(k=k, g=g, inputdim=x.shape[1], hidden=layers[0]).float()\n",
    "else: model = CoxSubgroupMixture(k=k, g=g, inputdim=x.shape[1]).float()\n",
    "\n",
    "(model, breslow_splines), losses = train(model, train_data, val_data, \n",
    "                                     epochs=epochs, lr=lr, use_posteriors=True, \n",
    "                                     patience=patience, return_losses=True, bs=bs,\n",
    "                                     smoothing_factor=smoothing_factor)\n",
    "\n",
    "if return_model: return (model, breslow_splines) \n",
    "\n",
    "print(\"Treatment Effects:\", model.treatment_effect)\n",
    "\n",
    "zeta_probs_train = torch.exp(model(x, a)[0]).sum(dim=1).detach().numpy()\n",
    "zeta_train =  np.argmax(zeta_probs_train, axis=1)\n",
    "\n",
    "if use_cf_evaluation:\n",
    "treated_outcomes_train, control_outcomes_train, _, _ = _load_estimated_counterfactuals(cf_path) \n",
    "max_treat_idx = _find_subgroup(zeta_probs_train, counterfactual_outcomes=(treated_outcomes_train, control_outcomes_train), MAX=True)\n",
    "min_treat_idx = _find_subgroup(zeta_probs_train, counterfactual_outcomes=(treated_outcomes_train, control_outcomes_train), MAX=False)\n",
    "else:\n",
    "max_treat_idx = _find_subgroup(zeta_probs_train, factual_outcomes=(outcomes_train, interventions_train), MAX=True)\n",
    "min_treat_idx = _find_subgroup(zeta_probs_train, factual_outcomes=(outcomes_train, interventions_train), MAX=False)\n",
    "\n",
    "x, t, e, a = features_test.values, outcomes_test['time'].values, outcomes_test['event'].values, interventions_test.values\n",
    "x, t, e, a = _convert_to_torch(x, t, e, a) \n",
    "\n",
    "zeta_probs_test = torch.exp(model(x, a)[0]).sum(dim=1).detach().numpy()\n",
    "\n",
    "max_subgroup_probs = (zeta_probs_train[:, max_treat_idx], zeta_probs_test[:, max_treat_idx])\n",
    "min_subgroup_probs = (zeta_probs_train[:, min_treat_idx], zeta_probs_test[:, min_treat_idx])\n",
    "\n",
    "return max_subgroup_probs, min_subgroup_probs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78547a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the clustering phenotyper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bbd932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
